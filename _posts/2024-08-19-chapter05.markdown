---
layout: post
title:  "혼자 공부하는 머신러닝+딥러닝 Chapter 5 발표"
published: true
date:   2024-08-19 
categories:
    - Data Science
    - 혼자 공부하는 머신러닝+딥러닝
tags: KHUDA ML
---
# 5. 트리 알고리즘
## 5-1. 결정 트리
- **결정 트리**(**Decision Tree**):\
예 / 아니오에 대한 질문을 이어나가면서 정답을 찾아 학습하는 알고리즘
- **노드**(**node**):\
결정 트리를 구성하는 핵심 요소
- **루트 노드**(**root node**):\
결정 트리 맨 위의 노드
- **리프 노드**(**leaf node**):\
결정 트리 맨 아래 노드
- **부모 노드**(**parent node**):\
어떤 하나의 노드에 대하여 이 노드의 상위에 연결된 노드
- **자식 노드**(**child node**):\
어떤 하나의 노드에 대하여 이 노드의 하위에 연결된 노드\
> 판다스 데이터프레임의 유용한 메서드 2개
> - **info() 메서드**:\
> 데이터 프레임의 각 열의 데이터 타입과 누락한 데이터가 있는지 확인하는데 유용\
> - **describe() 메서드**:\
> 평균(mean), 표준편차(std), 최소(min), 최대(max), 중간값 (50%), 1분위사수(25%), 3분위사수(75%) 제공
{: .prompt-tip }

### 로지스틱 회귀로 와인 분류하기
```python
# 와인 데이터 준비
import pandas as pd
wine = pd.read_csv('https://bit.ly/wine-date')

# 넘파이 배열로 바꾸고 훈련 세트와 테스트 세트로 나누기
data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
target = wine['class'].to_numpy()
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42)

# StandardScaler 클래스를 사용하여 훈련 세트 전처리
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)

# 표준점수로 변환된 train_scaled와 test_scaled를 사용해 로지스틱 회귀 모델을 훈련
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(train_scaled, train_target)
print(lr.score(train_scaled, train_target))
print(lr.score(test_scaled, test_target))
```
`0.7808350971714451`
`0.7776923076923077`\
훈련 세트와 테스트 세트의 점수가 모두 낮으므로 모델이 다소 과소적합
### 결정 트리
결정 트리는 다음 그림과 같이 스무고개와 같다.\
![결정트리](/assets/img/결정트리1.png)\
사이킷런의 DecisionTreeClassifier 클래스를 사용해 결정 트리 모델 훈련해보기
```python
# 결정 트리 모델 훈련
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier()
dt.fit(train_scaled, train_target)
print(dt.score(train_scaled, train_target))
print(dt.score(test_scaled, test_target))
```
`0.996921300750433`
`0.8576923076923076`\
훈련 세트에 대한 점수가 엄청 높고 테스트 세트의 성능은 그에 비해 조금 낮으므로 과대적합된 모델
plot_tree() 함수를 사용해 이해하기 쉬운 트리 그림으로 출력하기
```python
# 트리 그림 출력
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree
plt.figure(figsize=(10, 7))
plot_tree(dt)
plt.show()
```
![결정트리2](/assets/img/결정트리.png)\
너무 복잡하므로 보기 간단하게 만들기
```python
plt.figure(figsize=(10, 7))

# 트리 깊이 제한(max_depth 매개변수)
# 노드 색 칠하기(filled 매개변수)
# 노드 이름 전달(feature_names 매개변수)
plot_tree(dt, max_depth=1, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()
```
![결정트리3](/assets/img/결정트리2-1.png)\
### 노드
노드는 다음과 같은 정보를 담고 있다.\
![노드정보](/assets/img/노드정보.png)\
> **여기서 gini란?**
> - **DecisionTreeClassifier 클래스에서 노드의 데이터 분할 기준을 정하는 criterion 매개변수의 기본값**
> - **지니 불순도(Gini impurity)를 의미**
> - **지니 불순도 계산법** : \
> **1 - (음성 클래스 비율<sup>2</sup> + 양성 클래스 비율<sup>2</sup>)**
> - **정보 이득 : 부모와 자식 노드 사이의 불순도의 차이**
>-  **정보 이득 계산법** : \
> **부모의 불순도-(왼쪽 노드 샘플 수/부모의 샘플 수)x왼쪽 노드 불순도-(오른쪽 노드 샘플 수/부모의 샘플 수)x오른쪽 노드 불순도**
> - **엔트로피 불순도 : 사이킷런에는 criterion='entropy'로 지정하여 사용하는 또 다른 불순도 기준**
> - **엔트로피 불순도 계산법** : \
> **-음성 클래스 비율 x log<sub>2</sub>(음성 클래스 비율) - 양성 클래스 비율x log<sub>2</sub>(양성 클래스 비율)**
{: .prompt-tip }
### 가지치기
무작정 끝까지 자라나는 트리가 만들어지는 것을 방지하기 위해 트리의 최대 깊이를 지정
```python
dt = DecisionTreeClassifier(max_depth=3, random_state=42)
dt.fit(train_scaled, train_target)
print(dt.score(train_scaled, train_target))
print(dt.score(test_scaled, test_target))
```
`0.8454877814123533`
`0.8415384615384616`\
훈련 세트의 성능은 낮아졌지만 테스트 세트의 성능은 거의 그대로
```python
# 트리 그래프 그리기
plt.figure(0, figsize=(20, 15))
plot_tree(dt, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()
```
![결정트리4](/assets/img/결정트리4-2.png)\
결정 트리는 어떤 특성이 가장 유용한지 나타내는 **특성 중요도**를 계산해줌.\
위 트리의 루트 노드와 깊이 1에서 당도를 사용했기 때문에 당도가 가장 유용한 특성 중 하나일 것으로 추측됨.
- 특성 중요도 : \
결정 트리에 사용된 특성이 불순도를 감소하는데 기여한 정도
- 특성 중요도 계산법 :\
각 노드의 정보 이득과 전체 샘플에 대한 비율을 곱한 후 특성별로 더하여 계산

## 5-2. 교차 검증과 그리드 서치
### 검증 세트(validation set)
테스트 세트를 사용하지 않으면 모델의 과대적합/과소적합 여부를 판단하기 어려움\
이 때, 훈련 세트를 또 나눈 데이터인 **검증 세트**를 사용
- 검증 세트 사용법
1. 훈련 세트로 모델을 훈련, 검증 세트로 모델을 평가
2. 훈련 세트와 검증세트를 합쳐 모델을 훈련, 테스트 세트로 모델을 평가

![검증세트](/assets/img/검증%20세트.png)\
### 교차 검증(cross validation)
많은 데이터를 훈련에 사용할 수록 좋은 모델이 만들어지지만 검증 세트를 만들 때 훈련 세트가 줄어듦\
이 때, **교차 검증**을 이용하면 안정적인 검증 점수를 얻고 더 많은 데이터 사용 가능
- 3-폴드 교차 검증 : 훈련 세트를 세 부분으로 나눠서 교차 검증을 수행하는 것
- 보통 5-폴드 교차 검증이나 10-폴드 교차 검증을 많이 사용(데이터의 80~90%까지 훈련에 사용할 수 있기 때문)
> 3-폴드 교차 검증 그림
{: .prompt-info }

![교차검증](/assets/img/교차검증.png)\
cross_validate() : 교차 검증 함수
```python
# 와인 데이터 불러오기
import pandas as pd
wine = pd.read_csv('https://bit.ly/wine-date')

#class 열 타깃 사용, 나머지 열은 특성 배열에 저장
data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
target = wine['class'].to_numpy()

#훈련 세트와 테스트 세트 나누기
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42)

#교차 검증 함수 사용
from sklearn.model_selection import cross_validate
scores = cross_validate(dt, train_input, train_target)
print(scores)
```
`{'fit_time': array([0.01305008, 0.01255631, 0.00827003, 0.00501585, 0.00501204]), 'score_time': array([0.00238919, 0.00131226, 0.00170016, 0.00133657, 0.00118709]), 'test_score': array([0.84230769, 0.83365385, 0.84504331, 0.8373436 , 0.8479307 ])}`
- 이 함수는 fit_time, score_time, test_score 키를 가진 딕셔너리를 반환함
- 처음 2개의 키는 모델을 훈련하는 시간과 검증하는 시간 의미
- 각 키마다 5개의 숫자가 담겨있는데 cross_validate() 함수가 기본적으로 5-폴드 교차 검증을 수행하기 때문
```python
import numpy as np
print(np.mean(scores['test_score']))
```
`0.8412558303102096`

### 하이퍼파라미터 튜닝
- 모델 파라미터 : 머신러닝 모델이 학습하는 파라미터
- 하이퍼파라미터 : 모델이 학습할 수 없어 사용자가 지정해야만 하는 파라미터
- 하이퍼파라미터 튜닝 작업 :
1. 라이브러리가 제공하는 기본값을 그대로 사용해 모델 훈련
2. 검증 세트의 점수나 교차 검증을 통해서 매개변수를 조금씩 바꿔 봄
- max_depth 매개변수와 min_samples_split 매개변수를 동시에 바꿔가며 최적의 값을 찾아야함
- 사이킷런에서 제공하는 **그리드 서치를 사용**

그리드서치 사용 과정
1. 탐색할 매개변수 지정
2. 훈련 세트에서 그리드 서치를 수행하여 최상의 평균 검증 점수가 나오는 매개변수 조합 찾기
3. 그리드 서치는 최상의 매개변수에서 전체 훈련 세트를 사용해 최종 모델을 훈련

- 그리드 서치 코드
```python
from sklearn.model_selection import GridSearchCV

# 파라미터 딕셔너리
params = {'min_impurity_decrease': np.arange(0.0001, 0.001, 0.0001),
          'max_depth': range(5, 20, 1),
          'min_samples_split': range(2, 100, 10)
          }
# 그리드 서치 진행 및 학습
gs = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs=-1)
gs.fit(train_input, train_target)

# 최적의 모델을 통해 평가
dt = gs.best_estimator_
print(dt.score(train_input, train_target))

# 최적의 모델 파라미터
print(gs.best_params_)
>>> {'max_depth': 14, 'min_impurity_decrease': 0.0004, 'min_samples_split': 12}

# 교차 검증의 최대 점수
print(np.max(gs.cv_results_['mean_test_score']))
>>> 0.8683
```

### 랜덤 서치
- 매개변수 값의 목록을 전달하는 것이 아니라 매개변수를 샘플링할 수 있는 확률 분포 객체를 전달
- 연속된 매개변수 값을 탐색할 때 유용

-랜덤 서치 코드
```python
# 랜덤 서치
from scipy.stats import uniform, randint

# 파라미터 딕셔너리
params = {'min_impurity_decrease': uniform(0.0001, 0.001),
          'max_depth': randint(20, 50),
          'min_samples_split': randint(2, 25),
          'min_samples_leaf': randint(1, 25),
          }

# 랜덤 서치 진행 및 학습
from sklearn.model_selection import RandomizedSearchCV

gs = RandomizedSearchCV(DecisionTreeClassifier(random_state=42), params,
                        n_iter=100, n_jobs=-1, random_state=42)
gs.fit(train_input, train_target)

# 최적의 모델 파라미터
print(gs.best_params_)
>>> {'max_depth': 39, 'min_impurity_decrease': 0.00034102546602601173, 'min_samples_leaf': 7, 'min_samples_split': 13}

# 교차 검증의 최대 점수
print(np.max(gs.cv_results_['mean_test_score']))
>>> 0.8695

# 최적의 모델을 통해 평가
dt = gs.best_estimator_
print(dt.score(test_input, test_target))
>>> 0.86
```

#### 5-3. 트리의 앙상블