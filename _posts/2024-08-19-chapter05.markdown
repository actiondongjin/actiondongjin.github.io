---
layout: post
title:  "혼자 공부하는 머신러닝+딥러닝 Chapter 5 발표"
published: true
date:   2024-08-19 
categories:
    - Data Science
    - 혼자 공부하는 머신러닝+딥러닝
tags: KHUDA ML
---
# 5. 트리 알고리즘
## 5-1. 결정 트리
**결정 트리**(**Decision Tree**):\
예 / 아니오에 대한 질문을 이어나가면서 정답을 찾아 학습하는 알고리즘
**노드**(**node**):\
결정 트리를 구성하는 핵심 요소
**루트 노드**(**root node**):\
결정 트리 맨 위의 노드
**리프 노드**(**leaf node**):\
결정 트리 맨 아래 노드
**부모 노드**(**parent node**):\
어떤 하나의 노드에 대하여 이 노드의 상위에 연결된 노드
**자식 노드**(**child node**):\
어떤 하나의 노드에 대하여 이 노드의 하위에 연결된 노드
> 판다스 데이터프레임의 유용한 메서드 2개\
> info() 메서드:\
> 데이터 프레임의 각 열의 데이터 타입과 누락한 데이터가 있는지 확인하는데 유용\
> describe() 메서드:\
> 평균(mean), 표준편차(std), 최소(min), 최대(max), 중간값 (50%), 1분위사수(25%), 3분위사수(75%) 제공
{: .prompt-tip }

### 로지스틱 회귀로 와인 분류하기
```python
# 와인 데이터 준비
import pandas as pd
wine = pd.read_csv('https://bit.ly/wine-date')

# 넘파이 배열로 바꾸고 훈련 세트와 테스트 세트로 나누기
data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
target = wine['class'].to_numpy()
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42)

# StandardScaler 클래스를 사용하여 훈련 세트 전처리
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)

# 표준점수로 변환된 train_scaled와 test_scaled를 사용해 로지스틱 회귀 모델을 훈련
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(train_scaled, train_target)
print(lr.score(train_scaled, train_target))
print(lr.score(test_scaled, test_target))
```
`0.7808350971714451`
`0.7776923076923077`\
훈련 세트와 테스트 세트의 점수가 모두 낮으므로 모델이 다소 과소적합
### 결정 트리
결정 트리는 다음 그림과 같이 스무고개와 같다.\
![결정트리](/assets/img/결정트리1.png)\
사이킷런의 DecisionTreeClassifier 클래스를 사용해 결정 트리 모델 훈련해보기
```python
# 결정 트리 모델 훈련
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier()
dt.fit(train_scaled, train_target)
print(dt.score(train_scaled, train_target))
print(dt.score(test_scaled, test_target))
```
`0.996921300750433`
`0.8576923076923076`\
훈련 세트에 대한 점수가 엄청 높고 테스트 세트의 성능은 그에 비해 조금 낮으므로 과대적합된 모델
plot_tree() 함수를 사용해 이해하기 쉬운 트리 그림으로 출력하기
```python
# 트리 그림 출력
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree
plt.figure(figsize=(10, 7))
plot_tree(dt)
plt.show()
```
![결정트리2](/assets/img/결정트리.png)\
너무 복잡하므로 보기 간단하게 만들기
```python
plt.figure(figsize=(10, 7))

# 트리깊이 제한(max_depth매개변수)
# 노드 색 칠하기(filled 매개변수)
# 노드 이름 전달(feature_names 매개변수)
plot_tree(dt, max_depth=1, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()
```
![결정트리3](/assets/img/결정트리2-1.png)\
### 노드
노드는 다음과 같은 정보를 담고 있다.\
![노드정보](/assets/img/노드정보.png)\
> **여기서 gini란?**\
> gini : DecisionTreeClassifier 클래스의 노드에서 데이터를 분할할 기준을 정하는 criterion 매개변수의 기본값으로,\
> 지니 불순도(Gini impurity)를 의미\
> 지니 불순도 계산법 : \
> 1 - (음성 클래스 비율<sup>2</sup> + 양성 클래스 비율<sup>2</sup>)\
> 정보 이득 : 부모와 자식 노드 사이의 불순도의 차이\
> 정보 이득 계산법 : \
> 부모의 불순도-(왼쪽 노드 샘플 수/부모의 샘플 수)x왼쪽 노드 불순도-(오른쪽 노드 샘플 수/보무의 샘플 수)x오른쪽 노드 불순도\
> 엔트로피 불순도 : 사이킷런에는 criterion='entropy'로 지정하여 사용하는 또 다른 불순도 기준\
> 엔트로피 불순도 계산법 : \
> -음성 클래스 비율 x log<sub>2</sub>(음성 클래스 비율) - 양성 클래스 비율x log<sub>2</sub>(양성 클래스 비율)\
{: .prompt-tip }