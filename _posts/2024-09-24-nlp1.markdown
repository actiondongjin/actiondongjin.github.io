---
layout: post
title:  "NLP 2주차"
published: true
date:   2024-09-11 
categories:
    - Data Science
    - NLP
tags: KHUDA NLP
---
# Make More
- 주어진 데이터를 기반으로 더 많은 것을 생성하는 프로젝트인 Make More
- 예시 데이터셋 names.txt에는 약 32,000개의 이름이 포함
- 해당 데이터셋을 학습하면, Make More는 독특한 이름같은 고유한 이름을 생성
- 모델은 문자 단위로 데이터를 처리하며, 각 이름을 개별 문자 시퀀스로 다룸
- 시퀀스에서 다음 문자를 예측하는 여러 신경망 모델을 구현하는데, 이 과정에서 바이그램, 다층 퍼셉트론, 순환 신경망, 최신 트랜스포머를 포함한 다양한 모델이 사용
- 최종 목표는 GPT-2와 유사한 성능의 트랜스포머를 만드는 것

# 바이그램
- 자연어 처리 중 가장 기본적인 개념은 시퀀스를 만들어내는 작업
- 보통 고정된 길이의 연속된 토큰(Token) 시퀀스를 N-gram이라고 함
>- 유니그램 : 토큰 1개로 이루어진 연속된 시퀀스
>- 바이그램 : 토큰 2개로 이루어진 연속된 시퀀스
>- N-그램 : 토큰 N개로 이루어진 연속된 시퀀스

# 내용 정리
- 언어 모델링 도구인 **Make More**는 고유한 이름 생성을 지원
- 바이그램 모델의 문자 수열 예측은 모델링의 기초가 됨
- 모델의 **가능도**를 최대화 하는 것이 학습의 핵심
- 최소 제곱법과 통계 모델링에서는 가능도를 사용하여 모델이 학습한 전체 데이터 집합의 확률을 나타냄
- 가능도는 확률들의 곱으로, 모델이 학습 시 이 값이 가능한 최대가 되어야 함
- 하지만 이 확률들의 곱을 계산하기 어렵기 때문에, 일반적으로 **로그 가능도**를 사용
- 뉴럴 네트워크를 통한 추가 확장성은 언어 모델의 성능을 높임
- 모델 품질을 보장하기 위해 정규화와 스무딩이 중요